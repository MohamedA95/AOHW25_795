/******************************************************************************
 *  Copyright (c) 2019, Xilinx, Inc.
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions are met:
 *
 *  1.  Redistributions of source code must retain the above copyright notice,
 *     this list of conditions and the following disclaimer.
 *
 *  2.  Redistributions in binary form must reproduce the above copyright
 *      notice, this list of conditions and the following disclaimer in the
 *      documentation and/or other materials provided with the distribution.
 *
 *  3.  Neither the name of the copyright holder nor the names of its
 *      contributors may be used to endorse or promote products derived from
 *      this software without specific prior written permission.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 *  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 *  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 *  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
 *  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 *  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 *  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 *  OR BUSINESS INTERRUPTION). HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 *  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 *  OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 *  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 ******************************************************************************/

/******************************************************************************
 *
 *  Authors: Giulio Gambardella <giuliog@xilinx.com>
 *           Tobias Alonso <tobiasa@xilinx.com>
 *  \file pool.hpp
 *
 *  This file defines the pool activations
 *
 ******************************************************************************/

#ifndef POOL_HPP
#define POOL_HPP

#include "interpret.hpp"

/*!
 * \brief PoolFunction: General contract for pool functions.
 *
 * This class itself has no formal significance for the implementation
 * of the pool function. It provides a guidence for specific pool function to be used in Pool_batch
 *
 * \tparam TA Datatype of the internal accumulation in the pool function
 * \tparam TO Datatype of the output generated by the pool function
 * \tparam size Additional optional unsigned parameter to be used in pool or activate
 *
 */
template <typename TA, typename TO, unsigned size>
class PoolFunction
{
public:
  /*!
   * \brief init: initialization function returning the datatype for the accumulators
   */
  TA init(void) const
  {
#pragma HLS INLINE
    return TA(0);
  }

  /*!
   * \brief pool: computes the pooling algorithm (e.g., max, avg, sum)
   *
   * \param input Input value to be used in the pool function
   * \param accu  Value already computed in previous iterations
   */
  TA pool(TA const &input, TA const &accu) const;
  /*!
   * \brief activate: compute the output of pooling algorithm (e.g., max, avg, sum)
   *
   * \param accu Value already computed in previous iterations
   */
  TO activate(TA const &accu) const;
};

/*!
 * \brief MaxPoolFunction: Implementing max pool
 *
 * This class inherits from the generic Poolfunction to implement Max Pool
 *
 * \tparam T Datatype of the input value and the accu value containing the previously computed max
 * \tparam size Unused
 *
 */
template <typename T, unsigned size>
class MaxPoolFunction : public PoolFunction<T, T, size>
{
public:
  T init(void) const
  {
#pragma HLS INLINE
    const T T_MIN_VAL = (T(-1) < 0) ? 1 << (T::width - 1) : 0;
    return T_MIN_VAL;
  }
  /*!
   * \brief pool: computes the max value
   *
   * \param input Input value to be used in the max pool function
   * \param accu  Max value already computed in previous iterations
   */
  T pool(T const &input, T const &accu) const
  {
#pragma HLS INLINE
    return std::max(input, accu);
  }
  /*!
   * \brief activate: compute the output of the max pooling algorithm
   *
   * \param accu Max value already computed and returned
   */
  T activate(T const &accu) const
  {
#pragma HLS INLINE
    return accu;
  }
};

/*!
 * \brief AvgPoolFunction: Implementing avg pool
 *
 * This class inherits from the generic Poolfunction to implement Average Pool
 *
 * \tparam TA Datatype of the internal accumulation in the avg pool function
 * \tparam TO Datatype of the output generated by the avg pool function
 * \tparam size Value used as divisor on the accumulator to generate output
 *
 */
template <typename TA, typename TO, unsigned size>
class AvgPoolFunction : public PoolFunction<TA, TO, size>
{
public:
  /*!
   * \brief pool: computes the sum
   *
   * \param input Input value to be used in the avg pool function
   * \param accu  Accumulation value already computed in previous iterations
   */
  TA pool(TA const &input, TA const &accu) const
  {
#pragma HLS INLINE
    return std::plus<TA>()(input, accu);
  }
  /*!
   * \brief activate: compute the output of the avg pooling algorithm
   *
   * \param accu Accumulation value already computed in previous iterations
   */
  TO activate(TA const &accu) const
  {
#pragma HLS INLINE
    return (accu / size);
  }
};

/*!
 * \brief AccPoolFunction: Implementing accumulation pool
 *
 * This class inherits from the generic Poolfunction to implement accumulation Pool
 *
 * \tparam TA Datatype of the internal accumulation in the avg pool function

 * \tparam size Unused
 *
 */
template <typename TA, unsigned size>
class AccPoolFunction : public PoolFunction<TA, TA, size>
{
public:
  /*!
   * \brief pool: computes the sum
   *
   * \param input Input value to be used in the avg pool function
   * \param accu  Accumulation value already computed in previous iterations
   */
  TA pool(TA const &input, TA const &accu) const
  {
#pragma HLS INLINE
    return std::plus<TA>()(input, accu);
  }
  /*!
   * \brief activate: compute the output of the max pooling algorithm
   *
   * \param accu Accumulation value already computed in previous iterations
   */
  TA activate(TA const &accu) const
  {
#pragma HLS INLINE
    return accu;
  }
};

/*!
 * \brief QuantAvgPoolFunction: Implementing avg pool with shift instead of
 * division
 *
 * This class inherits from the generic Poolfunction to implement Average Pool
 * with shift instead of division
 *
 * \tparam TA Datatype of the internal accumulation in the quant avg pool function
 * \tparam TO Datatype of the output generated by the quant avg pool function
 * \tparam size Number of right shifts applied to generate output
 *
 */
template <typename TA, typename TO, unsigned size>
class QuantAvgPoolFunction : public PoolFunction<TA, TO, size>
{
public:
  /*!
   * \brief pool: computes the sum
   *
   * \param input Input value to be used in the avg pool function
   * \param accu  Accumulation value already computed in previous iterations
   */
  TA pool(TA const &input, TA const &accu) const
  {
#pragma HLS INLINE
    return input + accu;
  }
  /*!
   * \brief activate: compute the output of the quant avg pooling algorithm
   *
   * \param accu Accumulation value already computed in previous iterations
   */
  TO activate(TA const &accu) const
  {
#pragma HLS INLINE
    return TO(accu >> size);
  }
};

/**
 * \brief Adaptive_Avg_Pool_2d function
 *
 * The function performs an Adaptive_Avg_Pool_2d pool operation on the input stream.
 *
 * \tparam Channels   Number of channels - input is equal to output -
 * \tparam PE         Number of channels in the pool layer computed in parallel - currently only 1 is supported -
 * \tparam IFM_Dim    Width of the input feature map
 * \tparam OFM_Dim    Width of the output feature map - (OFM) currently only 1 is supported
 * \tparam Act_dtype  DataType of the activation
 * \tparam Acc_dtype  DataType of the accumulator
 * \tparam TI         DataType of the input stream - safely deducible from the paramaters
 * \tparam TO         DataType of the output stream - safely deducible from the paramaters
 *
 * \param in          Input stream
 * \param out         Output stream
 * \param reps        Number of time the function has to be repeatedly executed (e.g. number of images)
 */
template <
    unsigned Channels, unsigned PE, unsigned IFM_Dim, unsigned OFM_Dim,
    typename Act_dtype, typename Acc_dtype, typename TM, short min_val, short max_val,
    typename TI, typename TO>
void Adaptive_Avg_Pool_2d(hls::stream<TI> &in,
                          hls::stream<TO> &out,
                          const TM m0,
                          const Act_dtype inp_zero_point,
                          const Act_dtype out_zero_point,
                          const int reps)
{
  static_assert(OFM_Dim == 1, ""); //"Only OFM_Dim=1 is supported
  static_assert(Channels % PE == 0, ""); //Channels must be a multiple of PE
  unsigned const SF = IFM_Dim * IFM_Dim;  // the dtype here has to support negative values so the division is correct

  Acc_dtype accu[PE][Channels / PE];
#pragma HLS ARRAY_PARTITION variable = accu complete dim = 1
  for (unsigned r = 0; r < reps; r++)
  {
    for (unsigned i = 0; i < SF; i++)
    {
      for (unsigned c = 0; c < Channels / PE; c++)
      {
#pragma HLS PIPELINE style=flp II=1
        TI pixel_slice;

        // Threshold Initialisation
        if (i == 0)
        {
          for (unsigned pe = 0; pe < PE; pe++)
          {
#pragma HLS UNROLL
            accu[pe][c] = 0;
          }
        }

        for (unsigned pe = 0; pe < PE; pe++)
        {
#pragma HLS UNROLL
          pixel_slice = in.read();
          Act_dtype const casted_slice = *reinterpret_cast<Act_dtype const *>(&pixel_slice);
          accu[pe][c] += casted_slice - inp_zero_point;

          // keep track of which folded synapse/neuron we are processing
          if (i == SF - 1)
          {
            TO outElem;
            ap_fixed<32,10> tmp_res = accu[pe][c] * m0;
            tmp_res += out_zero_point;
            tmp_res = hls::nearbyint(tmp_res);
            if (tmp_res > max_val) {
              tmp_res = max_val;
            } else if (tmp_res < min_val) {
              tmp_res = min_val;
            }
            Act_dtype casted_res = tmp_res;
            outElem = *reinterpret_cast<TO *>(&casted_res);
            out.write(outElem);
          }
        }
      }
    }
  }
}

#endif
